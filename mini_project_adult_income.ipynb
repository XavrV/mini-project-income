{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ### BLOQUE 1: Introduccion, marco teorico y todo eso\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ### BLOQUE 2: Importar Gente y Configuraci\u00f3n Inicial\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Visualizaci\u00f3n\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "\n",
        "# MLflow para tracking de experimentos\n",
        "import mlflow\n",
        "import mlflow.sklearn\n",
        "import json\n",
        "\n",
        "# scikit-learn: preprocesado, pipeline, modelos y m\u00e9tricas\n",
        "import sklearn\n",
        "from sklearn.model_selection    import train_test_split, GridSearchCV\n",
        "from sklearn.impute             import SimpleImputer\n",
        "from sklearn.preprocessing      import OneHotEncoder, OrdinalEncoder, StandardScaler, RobustScaler, MinMaxScaler\n",
        "from sklearn.base               import BaseEstimator, TransformerMixin\n",
        "from sklearn.compose            import ColumnTransformer\n",
        "from sklearn.pipeline           import Pipeline\n",
        "from sklearn.linear_model       import LogisticRegression, LinearRegression, Lasso, Ridge\n",
        "from sklearn.tree               import DecisionTreeClassifier, DecisionTreeRegressor\n",
        "from sklearn.ensemble           import RandomForestClassifier, RandomForestRegressor\n",
        "from sklearn.metrics            import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    roc_auc_score, mean_squared_error, r2_score\n",
        ")\n",
        "\n",
        "import scipy\n",
        "from scipy.stats.mstats import winsorize\n",
        "\n",
        "# Warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configuraci\u00f3n global reproducible\n",
        "np.random.seed(42)\n",
        "RANDOM_STATE = 42\n",
        "\n",
        "# Mostrar todas las columnas y formateo de floats\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.float_format', lambda x: '%.2f' % x)\n",
        "\n",
        "# Estilos visuales por defecto\n",
        "plt.style.use('default')\n",
        "sns.set_palette('deep')\n",
        "\n",
        "# Mostrar versiones para reproducibilidad\n",
        "print(\"Python:\", sys.version)\n",
        "print(\"Pandas:\", pd.__version__)\n",
        "print(\"NumPy:\", np.__version__)\n",
        "print(\"Matplotlib:\", plt.matplotlib.__version__)\n",
        "print(\"Seaborn:\", sns.__version__)\n",
        "print(\"MLflow:\", mlflow.__version__)\n",
        "print(\"Scikit-learn:\", sklearn.__version__)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "config = { ## \n",
        "    \"path\": \"adult.csv\",\n",
        "    \"target\": \"income\",\n",
        "    \"target_mapping\": {\"<=50K\": 0, \">50K\": 1},\n",
        "    \"expected_columns\": [\n",
        "        'age', 'workclass', 'fnlwgt', 'education', 'education.num', 'marital.status',\n",
        "        'occupation', 'relationship', 'race', 'sex',\n",
        "        'capital.gain', 'capital.loss', 'hours.per.week', 'native.country', 'income'\n",
        "    ],\n",
        "\n",
        "    \"features\": [\n",
        "        'age', 'workclass', 'education.num', 'marital.status', 'occupation',\n",
        "        'relationship', 'race', 'sex', 'capital.gain', 'capital.loss',\n",
        "        'hours.per.week', 'native.country'\n",
        "    ],\n",
        "    \"drop_features\": ['education', 'fnlwgt'],\n",
        "\n",
        "    \"num_features\": ['age', 'education.num', 'capital.gain', 'capital.loss', 'hours.per.week'],\n",
        "    \"cat_features\": [\n",
        "        'workclass', 'marital.status', 'occupation',\n",
        "        'relationship', 'race', 'sex', 'native.country'\n",
        "    ],\n",
        "\n",
        "    # Outliers (tuneable)\n",
        "    \"outliers\": {\n",
        "        \"strategy\": \"clip\",  # o \"none\"\n",
        "        \"params\": {\"pmin\": 0.01, \"pmax\": 0.99}  # Para tunear desde param_grid\n",
        "    },\n",
        "\n",
        "    # Imputaci\u00f3n\n",
        "    \"imputation\": {\n",
        "        \"num_method\": \"median\",\n",
        "        \"cat_method\": \"most_frequent\",\n",
        "        # \"cat_fill_value\": \"Missing\"\n",
        "    },\n",
        "\n",
        "    # Agrupamiento de categor\u00edas poco frecuentes\n",
        "    \"grouping\": {\n",
        "        \"workclass\": {\"min_freq\": 3000, \"other_label\": \"Other\"},\n",
        "        \"marital.status\": {\"min_freq\": 2000, \"other_label\": \"Other\"},\n",
        "        \"occupation\": {\"min_freq\": 2000, \"other_label\": \"Other\"},\n",
        "        \"relationship\": {\"min_freq\": 4000, \"other_label\": \"Other\"},\n",
        "        \"race\": {\"min_freq\": 500, \"other_label\": \"Other\"},\n",
        "        \"native.country\": {\"min_freq\": 1000, \"other_label\": \"Other\"}\n",
        "    },\n",
        "\n",
        "\n",
        "    \"scaling\": \"standard\",   # O tun\u00e9ar en param_grid\n",
        "    \"encoding\": \"onehot\",    # \n",
        "    \n",
        "    \"split\": {\n",
        "        \"test_size\": 0.2,\n",
        "        \"random_state\": 42\n",
        "    },\n",
        "\n",
        "    \"model\": {\n",
        "        \"type\": \"LogisticRegression\",\n",
        "        \"params\": {\"max_iter\": 1000, \"solver\": \"lbfgs\"}\n",
        "    },\n",
        "\n",
        "    \"mlflow\": {\n",
        "        \"tracking_uri\": \"file:./mlruns\",\n",
        "        \"experiment_name\": \"Mini_Project_ Adult_Income\"\n",
        "    }\n",
        "}\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ### BLOQUE 3: Carga, Validaci\u00f3n y Exploraci\u00f3n Inicial\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# [3.1] Carga del Dataset\n",
        "def load_dataset(path: str, encoding: str = \"utf-8\", sep: str = \",\") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Lee el dataset desde el path configurado.\n",
        "    Entradas:\n",
        "        - path: ruta del archivo CSV.\n",
        "        - encoding: encoding del archivo (por defecto 'utf-8').\n",
        "        - sep: separador de columnas (por defecto ',').\n",
        "    Salida:\n",
        "        - DataFrame con los datos cargados.\n",
        "    L\u00f3gica:\n",
        "        - Usa pandas.read_csv (try/except para error handling)\n",
        "        - Devuelve el DataFrame le\u00eddo o lanza excepci\u00f3n si falla\n",
        "    \"\"\"\n",
        "    try:\n",
        "        df = pd.read_csv(path, encoding=encoding, sep=sep)\n",
        "        # Reemplazar '?' por np.nan en TODO el DataFrame\n",
        "        df.replace('?', np.nan, inplace=True)\n",
        "\n",
        "        print(f\"[INFO] Dataset cargado correctamente desde {path} con shape {df.shape}\")\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR] Fallo al cargar dataset desde {path}: {e}\")\n",
        "        raise\n",
        "\n",
        "# [3.2] Validaci\u00f3n de Estructura y Tipos\n",
        "def validate_structure(df: pd.DataFrame, expected_cols: list) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Verifica que las columnas y tipos sean los esperados.\n",
        "    Entradas:\n",
        "        - df: DataFrame cargado.\n",
        "        - expected_cols: lista de nombres de columnas esperadas.\n",
        "    Salida:\n",
        "        - DataFrame con columnas renombradas/tipos corregidos si es necesario.\n",
        "    L\u00f3gica:\n",
        "        - Chequear que columnas == expected_cols (ignora orden)\n",
        "        - Loggear advertencia si faltan o sobran columnas\n",
        "        - (Opcional) Renombrar columnas si hay variantes conocidas\n",
        "        - (Opcional) Corregir tipos simples (ej: int, float, str)\n",
        "        - No modifica datos si todo est\u00e1 OK\n",
        "    \"\"\"\n",
        "    df_cols = set(df.columns)\n",
        "    exp_cols = set(expected_cols)\n",
        "    missing = exp_cols - df_cols\n",
        "    extra = df_cols - exp_cols\n",
        "\n",
        "    if missing:\n",
        "        print(f\"[WARN] Faltan columnas: {missing}\")\n",
        "    if extra:\n",
        "        print(f\"[WARN] Columnas extra no esperadas: {extra}\")\n",
        "\n",
        "    # Opcional: podr\u00edas renombrar columnas aqu\u00ed si sabes que a veces tienen un typo o espacio.\n",
        "    # Por ahora solo loggeamos.\n",
        "    # Chequeo r\u00e1pido de tipos (opcional):\n",
        "    # Ejemplo: asegurarse que num\u00e9ricas est\u00e9n como num\u00e9ricas, etc.\n",
        "\n",
        "    print(f\"[INFO] Columnas finales en el DataFrame: {list(df.columns)}\")\n",
        "    return df\n",
        "\n",
        "# [3.3] Exploraci\u00f3n R\u00e1pida del Dataset\n",
        "def quick_explore(df: pd.DataFrame) -> dict:\n",
        "    \"\"\"\n",
        "    Realiza una exploraci\u00f3n r\u00e1pida del dataset.\n",
        "    Entradas:\n",
        "        - df: DataFrame a explorar.\n",
        "    Salida:\n",
        "        - dict con res\u00famenes r\u00e1pidos:\n",
        "            - shape, info, describe, nulos por columna\n",
        "            - valores \u00fanicos por columna\n",
        "            - posibles outliers (min/max extremos)\n",
        "    L\u00f3gica:\n",
        "        - Compila los datos b\u00e1sicos en un dict para logging/diagn\u00f3stico\n",
        "        - No modifica el df\n",
        "    \"\"\"\n",
        "    summary = {\n",
        "        \"shape\": df.shape,\n",
        "        \"columns\": list(df.columns),\n",
        "        \"nans_per_col\": df.isnull().sum().to_dict(),\n",
        "        \"unique_values\": {col: df[col].nunique() for col in df.columns},\n",
        "        \"describe\": df.describe(include=\"all\").T,  # .T para que sea m\u00e1s legible\n",
        "        \"min_per_col\": df.select_dtypes(include='number').min().to_dict(),\n",
        "        \"max_per_col\": df.select_dtypes(include='number').max().to_dict()\n",
        "    }\n",
        "    print(\"[INFO] Resumen r\u00e1pido del dataset:\")\n",
        "    print(f\" - Shape: {summary['shape']}\")\n",
        "    print(f\" - Nulos por columna: {summary['nans_per_col']}\")\n",
        "    print(f\" - \u00danicos por columna: {summary['unique_values']}\")\n",
        "    return summary\n",
        "\n",
        "# [3.4] Chequeo de Nulos y Duplicados\n",
        "def check_nulls_duplicates(df: pd.DataFrame) -> dict:\n",
        "    \"\"\"\n",
        "    Revisa nulos y duplicados.\n",
        "    Entradas:\n",
        "        - df: DataFrame a analizar.\n",
        "    Salida:\n",
        "        - dict con:\n",
        "            - cantidad de nulos por columna\n",
        "            - cantidad de filas duplicadas\n",
        "    L\u00f3gica:\n",
        "        - df.isnull().sum()\n",
        "        - df.duplicated().sum()\n",
        "        - Retorna dict resumen para logging\n",
        "    \"\"\"\n",
        "    nans_per_col = df.isnull().sum().to_dict()\n",
        "    total_dupes = df.duplicated().sum()\n",
        "    summary = {\n",
        "        \"nans_per_col\": nans_per_col,\n",
        "        \"total_duplicates\": total_dupes\n",
        "    }\n",
        "    print(f\"[INFO] Nulos por columna: {nans_per_col}\")\n",
        "    print(f\"[INFO] Total filas duplicadas: {total_dupes}\")\n",
        "    return summary\n",
        "\n",
        "# [3.5] Descripci\u00f3n de Variables (Data Dictionary)\n",
        "def generate_data_dictionary(df: pd.DataFrame, target_col: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Genera un data dictionary resumido.\n",
        "    Entradas:\n",
        "        - df: DataFrame base.\n",
        "        - target_col: nombre de la columna target.\n",
        "    Salida:\n",
        "        - DataFrame/tabla con:\n",
        "            - nombre, tipo, rol (target/feature), notas/resumen si aplica\n",
        "    L\u00f3gica:\n",
        "        - Itera por columnas, define tipo (num/cat), rol (feature/target)\n",
        "        - Opcional: a\u00f1ade significado conocido o notas manuales\n",
        "    \"\"\"\n",
        "    data = []\n",
        "    for col in df.columns:\n",
        "        tipo = str(df[col].dtype)\n",
        "        rol = \"target\" if col == target_col else \"feature\"\n",
        "        notas = \"\"\n",
        "        # Podr\u00edas agregar descripciones/manuales aqu\u00ed si lo deseas\n",
        "        data.append({\n",
        "            \"name\": col,\n",
        "            \"type\": tipo,\n",
        "            \"role\": rol,\n",
        "            \"notes\": notas\n",
        "        })\n",
        "    dict_df = pd.DataFrame(data)\n",
        "    print(\"[INFO] Diccionario de datos generado.\")\n",
        "    return dict_df\n",
        "\n",
        "# [3.6] Guardado de Estado Inicial\n",
        "def save_initial_state(df: pd.DataFrame, path: str) -> None:\n",
        "    \"\"\"\n",
        "    Guarda el estado inicial del dataset limpio.\n",
        "    Entradas:\n",
        "        - df: DataFrame a guardar.\n",
        "        - path: ruta destino para el archivo de respaldo.\n",
        "    Salida:\n",
        "        - No retorna nada, solo guarda archivo y/o logs.\n",
        "    L\u00f3gica:\n",
        "        - Usa df.to_csv(path)\n",
        "        - (Opcional) Loggea confirmaci\u00f3n y shape guardado\n",
        "    \"\"\"\n",
        "    df.to_csv(path, index=False)\n",
        "    print(f\"[INFO] Estado inicial del dataset guardado en {path} (shape: {df.shape})\")\n",
        "\n",
        "# --- Helpers opcionales ---\n",
        "\n",
        "def log_shape(df: pd.DataFrame, msg: str = \"\") -> None:\n",
        "    print(f\"[INFO]{msg} Shape del DataFrame: {df.shape}\")\n",
        "\n",
        "def log_summary(info: dict, msg: str = \"\") -> None:\n",
        "    print(f\"[INFO]{msg}\")\n",
        "    for k, v in info.items():\n",
        "        print(f\"  {k}: {v}\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df = load_dataset(config[\"path\"])\n",
        "df = validate_structure(df, config[\"expected_columns\"])\n",
        "resumen = quick_explore(df)\n",
        "check_nulls_duplicates(df)\n",
        "data_dict = generate_data_dictionary(df, config[\"target\"])\n",
        "save_initial_state(df, \"adult_initial.csv\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ### BLOQUE 4: Procesamiento y Codificaci\u00f3n del Target\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# [4.1] Identificaci\u00f3n y Resumen del Target\n",
        "def inspect_target_values(df: pd.DataFrame, target_col: str) -> dict:\n",
        "    \"\"\"\n",
        "    Identifica y resume la columna target.\n",
        "    Entradas:\n",
        "        - df: DataFrame completo.\n",
        "        - target_col: nombre de la columna target ('income').\n",
        "    Salida:\n",
        "        - dict con:\n",
        "            - valores \u00fanicos y frecuencias absolutas/relativas\n",
        "            - tipo de datos\n",
        "            - posibles anomal\u00edas (nulos, valores inesperados)\n",
        "    \"\"\"\n",
        "    vals = df[target_col].value_counts(dropna=False)\n",
        "    freqs = df[target_col].value_counts(normalize=True, dropna=False)\n",
        "    n_nulos = df[target_col].isnull().sum()\n",
        "    tipos = df[target_col].dtype\n",
        "    resumen = {\n",
        "        \"unique_values\": vals.to_dict(),\n",
        "        \"relative_frequencies\": freqs.to_dict(),\n",
        "        \"n_missing\": n_nulos,\n",
        "        \"dtype\": str(tipos)\n",
        "    }\n",
        "    print(f\"[INFO] Valores \u00fanicos y frecuencias en '{target_col}': {resumen['unique_values']}\")\n",
        "    print(f\"[INFO] Frecuencias relativas: {resumen['relative_frequencies']}\")\n",
        "    print(f\"[INFO] Nulos en target: {n_nulos}\")\n",
        "    print(f\"[INFO] Tipo de datos en target: {tipos}\")\n",
        "    return resumen\n",
        "\n",
        "# [4.2] Limpieza y Normalizaci\u00f3n del Target\n",
        "def clean_normalize_target(df: pd.DataFrame, target_col: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Limpia y normaliza los valores del target.\n",
        "    Entradas:\n",
        "        - df: DataFrame original.\n",
        "        - target_col: columna target.\n",
        "    Salida:\n",
        "        - Nuevo DataFrame con target limpio (espacios, may\u00fasculas/min\u00fasculas, valores est\u00e1ndar).\n",
        "    L\u00f3gica:\n",
        "        - Elimina espacios en blanco, normaliza may\u00fasculas/min\u00fasculas\n",
        "        - Corrige typos frecuentes\n",
        "        - Elimina/marca como nulos los valores vac\u00edos/incorrectos\n",
        "        - Deja solo dos categor\u00edas v\u00e1lidas\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "    # 1. Limpiar espacios y may\u00fasculas/min\u00fasculas\n",
        "    df[target_col] = df[target_col].astype(str).str.strip().str.replace(\".\", \"\", regex=False)\n",
        "    df[target_col] = df[target_col].str.replace(\" \", \"\")  # Elimina todos los espacios\n",
        "    df[target_col] = df[target_col].str.replace(\"<=\", \"<=\", regex=False)\n",
        "    df[target_col] = df[target_col].str.replace(\">\", \">\", regex=False)\n",
        "    df[target_col] = df[target_col].str.replace(\"K\", \"K\", regex=False)\n",
        "    \n",
        "    # 2. Solo dejar los valores v\u00e1lidos ('<=50K', '>50K'), otros a NaN\n",
        "    valid_values = {\"<=50K\", \">50K\"}\n",
        "    df[target_col] = df[target_col].where(df[target_col].isin(valid_values), other=pd.NA)\n",
        "    n_invalid = df[target_col].isna().sum()\n",
        "    print(f\"[INFO] Target '{target_col}' normalizado. Valores fuera de ['<=50K', '>50K']: {n_invalid}\")\n",
        "    return df\n",
        "\n",
        "# [4.3] Visualizaci\u00f3n del Balance de Clases\n",
        "def plot_target_distribution(df: pd.DataFrame, target_col: str) -> None:\n",
        "    \"\"\"\n",
        "    Visualiza la distribuci\u00f3n de clases del target.\n",
        "    Entradas:\n",
        "        - df: DataFrame con target limpio.\n",
        "        - target_col: columna target.\n",
        "    Salida:\n",
        "        - No retorna nada, solo visualizaci\u00f3n y/o logging.\n",
        "    L\u00f3gica:\n",
        "        - Gr\u00e1fico de barras (countplot) con frecuencias absolutas y relativas\n",
        "        - Muestra tabla resumen\n",
        "    \"\"\"\n",
        "    fig, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
        "    # Gr\u00e1fico de cuentas absolutas\n",
        "    sns.countplot(x=target_col, data=df, ax=ax[0])\n",
        "    ax[0].set_title(\"Distribuci\u00f3n absoluta del target\")\n",
        "    # Gr\u00e1fico de cuentas relativas\n",
        "    rel_freq = df[target_col].value_counts(normalize=True)\n",
        "    rel_freq.plot(kind='bar', ax=ax[1])\n",
        "    ax[1].set_title(\"Distribuci\u00f3n relativa del target\")\n",
        "    plt.suptitle(f\"Distribuci\u00f3n del target: {target_col}\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    print(\"[INFO] Visualizaci\u00f3n de distribuci\u00f3n de clases mostrada.\")\n",
        "\n",
        "# [4.4] Codificaci\u00f3n del Target para Modelado\n",
        "def encode_target(df: pd.DataFrame, target_col: str, mapping: dict) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Codifica el target para modelado (binario: 0/1).\n",
        "    Entradas:\n",
        "        - df: DataFrame con target limpio.\n",
        "        - target_col: nombre de la columna target.\n",
        "        - mapping: dict de mapeo (ej: {'<=50K': 0, '>50K': 1}).\n",
        "    Salida:\n",
        "        - DataFrame con target codificado (tipo int/categorical).\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "    # Aplica el mapping (si hay valores fuera de mapping quedan como NaN)\n",
        "    df[target_col] = df[target_col].map(mapping)\n",
        "    n_nan = df[target_col].isna().sum()\n",
        "    if n_nan > 0:\n",
        "        print(f\"[WARN] {n_nan} filas tienen valores no mapeados tras codificar el target.\")\n",
        "    else:\n",
        "        print(f\"[INFO] Target codificado correctamente ({mapping})\")\n",
        "    df[target_col] = df[target_col].astype(\"Int64\")  # Soporta NaN\n",
        "    return df\n",
        "\n",
        "# [4.5] Logging y Registro del Estado del Target\n",
        "def log_target_status(df: pd.DataFrame, target_col: str, removed_rows: int) -> None:\n",
        "    \"\"\"\n",
        "    Registra el estado post-limpieza del target.\n",
        "    Entradas:\n",
        "        - df: DataFrame tras limpieza.\n",
        "        - target_col: nombre de la columna target.\n",
        "        - removed_rows: n\u00famero de filas eliminadas/cambiadas.\n",
        "    Salida:\n",
        "        - No retorna nada, solo logging (shape, resumen del target, filas afectadas).\n",
        "    \"\"\"\n",
        "    shape = df.shape\n",
        "    vals = df[target_col].value_counts(dropna=False).to_dict()\n",
        "    print(f\"[INFO] Estado post-limpieza del target '{target_col}':\")\n",
        "    print(f\"  - Shape actual: {shape}\")\n",
        "    print(f\"  - Valores codificados: {vals}\")\n",
        "    print(f\"  - Filas eliminadas/cambiadas: {removed_rows}\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "resumen_target = inspect_target_values(df, config[\"target\"])\n",
        "df = clean_normalize_target(df, config[\"target\"])\n",
        "plot_target_distribution(df, config[\"target\"])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df = encode_target(df, config[\"target\"], config[\"target_mapping\"])\n",
        "removed_rows = df[config[\"target\"]].isna().sum()\n",
        "log_target_status(df, config[\"target\"], removed_rows)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ### BLOQUE 5: Limpieza General y Revisi\u00f3n de Consistencias\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# [5.1] Eliminaci\u00f3n de Duplicados\n",
        "def drop_duplicates(df: pd.DataFrame) -> (pd.DataFrame, int):\n",
        "    \"\"\"\n",
        "    Elimina filas duplicadas completas.\n",
        "    Entradas:\n",
        "        - df: DataFrame original.\n",
        "    Salida:\n",
        "        - Nuevo DataFrame sin duplicados\n",
        "        - N\u00famero de filas eliminadas\n",
        "    L\u00f3gica:\n",
        "        - Usa df.drop_duplicates()\n",
        "        - Calcula cu\u00e1ntas filas se eliminaron\n",
        "    \"\"\"\n",
        "    initial_shape = df.shape\n",
        "    df_clean = df.drop_duplicates()\n",
        "    removed = initial_shape[0] - df_clean.shape[0]\n",
        "    print(f\"[INFO] Duplicados eliminados: {removed} (shape final: {df_clean.shape})\")\n",
        "    return df_clean, removed\n",
        "\n",
        "# [5.2] Manejo de NaNs / Valores Faltantes\n",
        "def analyze_missing_values(df: pd.DataFrame, target_col: str) -> dict:\n",
        "    \"\"\"\n",
        "    Analiza presencia y ubicaci\u00f3n de NaNs/valores faltantes.\n",
        "    Entradas:\n",
        "        - df: DataFrame.\n",
        "        - target_col: columna target.\n",
        "    Salida:\n",
        "        - dict con:\n",
        "            - resumen de nulos por columna\n",
        "            - cantidad total y ubicaci\u00f3n relevante\n",
        "    L\u00f3gica:\n",
        "        - df.isnull().sum()\n",
        "        - Registrar columnas con nulos, especialmente target\n",
        "        - No imputa ni elimina aqu\u00ed, solo documenta\n",
        "    \"\"\"\n",
        "    nans_per_col = df.isnull().sum().to_dict()\n",
        "    total_nans = sum(nans_per_col.values())\n",
        "    nans_in_target = nans_per_col.get(target_col, 0)\n",
        "    summary = {\n",
        "        \"nans_per_col\": nans_per_col,\n",
        "        \"total_nans\": total_nans,\n",
        "        \"nans_in_target\": nans_in_target\n",
        "    }\n",
        "    print(f\"[INFO] Nulos por columna: {nans_per_col}\")\n",
        "    print(f\"[INFO] Total nulos en dataset: {total_nans}\")\n",
        "    print(f\"[INFO] Nulos en target: {nans_in_target}\")\n",
        "    return summary\n",
        "\n",
        "# [5.3] Registro y Documentaci\u00f3n del Estado del Dataset\n",
        "def log_cleaning_status(df: pd.DataFrame, removed_duplicates: int, missing_summary: dict) -> None:\n",
        "    \"\"\"\n",
        "    Registra el estado tras limpieza de duplicados y an\u00e1lisis de nulos.\n",
        "    Entradas:\n",
        "        - df: DataFrame tras limpieza.\n",
        "        - removed_duplicates: filas eliminadas por duplicaci\u00f3n.\n",
        "        - missing_summary: dict con reporte de nulos.\n",
        "    Salida:\n",
        "        - No retorna nada, solo logging/documentaci\u00f3n.\n",
        "    \"\"\"\n",
        "    print(\"[INFO] Estado tras limpieza b\u00e1sica:\")\n",
        "    print(f\"  - Shape actual: {df.shape}\")\n",
        "    print(f\"  - Duplicados eliminados: {removed_duplicates}\")\n",
        "    print(f\"  - Nulos por columna: {missing_summary['nans_per_col']}\")\n",
        "    print(f\"  - Total nulos: {missing_summary['total_nans']}\")\n",
        "    print(f\"  - Nulos en target: {missing_summary['nans_in_target']}\")\n",
        "\n",
        "# [5.4] Correcci\u00f3n de Formatos y Tipos Evidentes\n",
        "def fix_column_types_and_formats(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Unifica formatos y corrige tipos de datos evidentes.\n",
        "    Entradas:\n",
        "        - df: DataFrame original.\n",
        "    Salida:\n",
        "        - DataFrame con formatos/tipos corregidos.\n",
        "    L\u00f3gica:\n",
        "        - Elimina espacios extra en strings\n",
        "        - Unifica may\u00fasculas/min\u00fasculas en categ\u00f3ricas\n",
        "        - Convierte columnas a num\u00e9ricas si corresponde\n",
        "    \"\"\"\n",
        "    df_clean = df.copy()\n",
        "    for col in df_clean.columns:\n",
        "        if df_clean[col].dtype == \"object\":\n",
        "            df_clean[col] = df_clean[col].astype(str).str.strip()\n",
        "    # (Opcional) Convertir columnas a num\u00e9ricas si tiene sentido\n",
        "    # Ejemplo: \"education.num\", \"capital.gain\", \"capital.loss\", \"hours.per.week\"\n",
        "    numeric_candidates = [\"education.num\", \"capital.gain\", \"capital.loss\", \"hours.per.week\", \"age\"]\n",
        "    for col in numeric_candidates:\n",
        "        if col in df_clean.columns:\n",
        "            df_clean[col] = pd.to_numeric(df_clean[col], errors=\"coerce\")\n",
        "    print(\"[INFO] Tipos y formatos corregidos para columnas relevantes.\")\n",
        "    return df_clean\n",
        "\n",
        "# [5.5] Revisi\u00f3n de Valores At\u00edpicos Evidentes (Superficial)\n",
        "def detect_gross_outliers(df: pd.DataFrame, num_cols: list) -> dict:\n",
        "    \"\"\"\n",
        "    Detecta posibles outliers groseros para an\u00e1lisis posterior.\n",
        "    Entradas:\n",
        "        - df: DataFrame.\n",
        "        - num_cols: lista de columnas num\u00e9ricas.\n",
        "    Salida:\n",
        "        - dict con rangos y valores fuera de rango observados.\n",
        "    L\u00f3gica:\n",
        "        - Calcula min/max para cada num_col\n",
        "        - Identifica valores fuera de rangos esperados (ej: negativos donde no deber\u00eda haber)\n",
        "        - No modifica datos, solo diagn\u00f3stico\n",
        "    \"\"\"\n",
        "    outliers = {}\n",
        "    for col in num_cols:\n",
        "        col_min = df[col].min()\n",
        "        col_max = df[col].max()\n",
        "        negative_count = (df[col] < 0).sum()\n",
        "        outliers[col] = {\n",
        "            \"min\": col_min,\n",
        "            \"max\": col_max,\n",
        "            \"n_negative\": int(negative_count)\n",
        "        }\n",
        "        print(f\"[INFO] Columna '{col}': min={col_min}, max={col_max}, valores negativos={negative_count}\")\n",
        "    return outliers\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df, removed_duplicates = drop_duplicates(df)\n",
        "missing_summary = analyze_missing_values(df, config[\"target\"])\n",
        "log_cleaning_status(df, removed_duplicates, missing_summary)\n",
        "df = fix_column_types_and_formats(df)\n",
        "outliers_report = detect_gross_outliers(df, config[\"num_features\"])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ### BLOQUE 6: An\u00e1lisis Exploratorio de Datos (EDA)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# [6.1] Estad\u00edsticos Descriptivos Generales\n",
        "def describe_numerical(df: pd.DataFrame, num_cols: list) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Calcula resumen estad\u00edstico para variables num\u00e9ricas.\n",
        "    \"\"\"\n",
        "    desc = df[num_cols].describe().T\n",
        "    print(\"[INFO] Estad\u00edsticos descriptivos (num\u00e9ricas):\")\n",
        "    print(desc)\n",
        "    return desc\n",
        "\n",
        "def describe_categorical(df: pd.DataFrame, cat_cols: list) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Calcula resumen descriptivo para variables categ\u00f3ricas.\n",
        "    \"\"\"\n",
        "    res = []\n",
        "    for col in cat_cols:\n",
        "        counts = df[col].value_counts()\n",
        "        res.append({\n",
        "            \"col\": col,\n",
        "            \"n_unique\": df[col].nunique(),\n",
        "            \"most_freq\": counts.idxmax() if not counts.empty else None,\n",
        "            \"freq_most_freq\": counts.max() if not counts.empty else None,\n",
        "            \"top_5\": counts.head(5).to_dict()\n",
        "        })\n",
        "    desc = pd.DataFrame(res)\n",
        "    print(\"[INFO] Estad\u00edsticos descriptivos (categ\u00f3ricas):\")\n",
        "    print(desc)\n",
        "    return desc\n",
        "\n",
        "# [6.2] Visualizaci\u00f3n de Variables Num\u00e9ricas\n",
        "def plot_numerical_distributions(df: pd.DataFrame, num_cols: list) -> None:\n",
        "    \"\"\"\n",
        "    Visualiza distribuciones de variables num\u00e9ricas.\n",
        "    \"\"\"\n",
        "    n = len(num_cols)\n",
        "    fig, axs = plt.subplots(1, n, figsize=(5*n, 4))\n",
        "    if n == 1:\n",
        "        axs = [axs]\n",
        "    for ax, col in zip(axs, num_cols):\n",
        "        sns.histplot(df[col], kde=True, ax=ax)\n",
        "        ax.set_title(f\"Histograma de {col}\")\n",
        "        ax.set_xlabel(col)\n",
        "        ax.set_ylabel(\"Frecuencia\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Boxplots por si quieres verlo m\u00e1s claro\n",
        "    fig, axs = plt.subplots(1, n, figsize=(5*n, 3))\n",
        "    if n == 1:\n",
        "        axs = [axs]\n",
        "    for ax, col in zip(axs, num_cols):\n",
        "        sns.boxplot(y=df[col], ax=ax)\n",
        "        ax.set_title(f\"Boxplot de {col}\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# [6.3] Visualizaci\u00f3n de Variables Categ\u00f3ricas\n",
        "def plot_categorical_distributions(df: pd.DataFrame, cat_cols: list, min_freq: int = 10) -> None:\n",
        "    \"\"\"\n",
        "    Visualiza conteos de categor\u00edas para variables categ\u00f3ricas.\n",
        "    \"\"\"\n",
        "    for col in cat_cols:\n",
        "        counts = df[col].value_counts()\n",
        "        # Agrupa categor\u00edas poco frecuentes\n",
        "        small_cats = counts[counts < min_freq].index\n",
        "        df_plot = df[col].replace(small_cats, \"Other\")\n",
        "        plt.figure(figsize=(8, 4))\n",
        "        sns.countplot(y=df_plot, order=df_plot.value_counts().index)\n",
        "        plt.title(f\"Distribuci\u00f3n de {col} (agrupando <{min_freq})\")\n",
        "        plt.show()\n",
        "\n",
        "# [6.4] Relaci\u00f3n Univariada de Features con el Target\n",
        "def plot_feature_target_relationship(df: pd.DataFrame, feature_cols: list, target_col: str) -> None:\n",
        "    \"\"\"\n",
        "    Explora relaciones univariadas entre features y el target.\n",
        "    \"\"\"\n",
        "    for col in feature_cols:\n",
        "        plt.figure(figsize=(7, 4))\n",
        "        if pd.api.types.is_numeric_dtype(df[col]):\n",
        "            sns.boxplot(x=target_col, y=col, data=df)\n",
        "            plt.title(f\"{col} por {target_col}\")\n",
        "        else:\n",
        "            cross = pd.crosstab(df[col], df[target_col], normalize='index')\n",
        "            cross.plot(kind=\"bar\", stacked=True)\n",
        "            plt.title(f\"{col} vs {target_col}\")\n",
        "            plt.xlabel(col)\n",
        "            plt.ylabel(\"Proporci\u00f3n\")\n",
        "            plt.legend(title=target_col)\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "# [6.5] Correlaciones y Relaciones Notables\n",
        "def correlation_analysis(df: pd.DataFrame, num_cols: list) -> (pd.DataFrame, object):\n",
        "    \"\"\"\n",
        "    Calcula y visualiza matriz de correlaciones para num\u00e9ricas.\n",
        "    \"\"\"\n",
        "    corr = df[num_cols].corr()\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(corr, annot=True, cmap='coolwarm', center=0)\n",
        "    plt.title(\"Matriz de correlaciones num\u00e9ricas\")\n",
        "    plt.show()\n",
        "    return corr\n",
        "\n",
        "def analyze_feature_redundancy(df: pd.DataFrame, feature_cols: list) -> dict:\n",
        "    \"\"\"\n",
        "    Detecta relaciones o redundancias notables entre features.\n",
        "    \"\"\"\n",
        "    redundancy = {}\n",
        "    corr = df[feature_cols].corr()\n",
        "    threshold = 0.95  # Sugerido, puedes parametrizarlo\n",
        "    for i in range(len(corr.columns)):\n",
        "        for j in range(i + 1, len(corr.columns)):\n",
        "            c1 = corr.columns[i]\n",
        "            c2 = corr.columns[j]\n",
        "            val = corr.iloc[i, j]\n",
        "            if abs(val) > threshold:\n",
        "                redundancy[(c1, c2)] = val\n",
        "    print(f\"[INFO] Pares de features con |correlaci\u00f3n| > {threshold}: {redundancy}\")\n",
        "    return redundancy\n",
        "\n",
        "# [6.6] Registro de Observaciones y Hallazgos\n",
        "def document_eda_findings(findings: dict, save_path: str = None) -> None:\n",
        "    \"\"\"\n",
        "    Documenta observaciones clave del EDA.\n",
        "    \"\"\"\n",
        "    print(\"[INFO] Hallazgos del EDA:\")\n",
        "    for k, v in findings.items():\n",
        "        print(f\"  - {k}: {v}\")\n",
        "    if save_path is not None:\n",
        "        with open(save_path, \"w\") as f:\n",
        "            for k, v in findings.items():\n",
        "                f.write(f\"{k}: {v}\\n\")\n",
        "        print(f\"[INFO] Hallazgos EDA guardados en {save_path}\")\n",
        "\n",
        "# [6.7] Logging y Exportaci\u00f3n de Resultados del EDA (Opcional)\n",
        "def save_eda_artifacts(plots: list, tables: list, out_dir: str) -> None:\n",
        "    \"\"\"\n",
        "    Guarda los gr\u00e1ficos y tablas relevantes generados durante el EDA.\n",
        "    \"\"\"\n",
        "    import os\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "    for i, fig in enumerate(plots):\n",
        "        fig_path = f\"{out_dir}/eda_plot_{i}.png\"\n",
        "        fig.savefig(fig_path)\n",
        "        print(f\"[INFO] Gr\u00e1fico guardado en {fig_path}\")\n",
        "    for i, table in enumerate(tables):\n",
        "        table_path = f\"{out_dir}/eda_table_{i}.csv\"\n",
        "        table.to_csv(table_path)\n",
        "        print(f\"[INFO] Tabla guardada en {table_path}\")\n",
        "    print(f\"[INFO] Artefactos EDA guardados en {out_dir}\")\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "desc_num = describe_numerical(df, config[\"num_features\"])\n",
        "desc_cat = describe_categorical(df, config[\"cat_features\"])\n",
        "plot_numerical_distributions(df, config[\"num_features\"])\n",
        "\n",
        "# 6.3 Visualizaci\u00f3n de Variables Categ\u00f3ricas (agrupando categor\u00edas poco frecuentes)\n",
        "plot_categorical_distributions(df, config[\"cat_features\"], min_freq=10)\n",
        "\n",
        "# 6.4 Relaci\u00f3n Univariada de Features con el Target\n",
        "plot_feature_target_relationship(df, config[\"features\"], config[\"target\"])\n",
        "\n",
        "# 6.5 Correlaciones y Relaciones Notables\n",
        "corr_matrix = correlation_analysis(df, config[\"num_features\"])\n",
        "redundancies = analyze_feature_redundancy(df, config[\"num_features\"])\n",
        "\n",
        "# 6.6 (Opcional) Documentaci\u00f3n de Hallazgos del EDA\n",
        "findings = {\n",
        "    \"desc_num\": desc_num.to_dict(),\n",
        "    \"desc_cat\": desc_cat.to_dict(),\n",
        "    \"correlaciones\": corr_matrix.to_dict(),\n",
        "    \"redundancias\": redundancies,\n",
        "    # Puedes agregar insights manualmente despu\u00e9s de revisar visualizaciones\n",
        "}\n",
        "document_eda_findings(findings, save_path=\"eda_findings.txt\")\n",
        "\n",
        "# 6.7 (Opcional) Guardado de artefactos de EDA\n",
        "# (Si quieres guardar gr\u00e1ficos/tablas, deber\u00edas recolectarlos en listas)\n",
        "# save_eda_artifacts(plots, tables, out_dir=\"eda_artifacts\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ### BLOQUE 7: Ingenier\u00eda y Selecci\u00f3n de Features YA COPIAIDO!!!!!\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# [7.1] An\u00e1lisis y Decisi\u00f3n sobre Features Redundantes o Irrelevantes\n",
        "def analyze_feature_redundancy_irrelevance(df: pd.DataFrame, features_info: dict) -> dict:\n",
        "    \"\"\"\n",
        "    Analiza y documenta features redundantes o irrelevantes.\n",
        "    \"\"\"\n",
        "    # features_info puede contener correlaciones, cardinalidades, etc.\n",
        "    redundantes = features_info.get(\"redundant\", [])\n",
        "    irrelevantes = features_info.get(\"irrelevant\", [])\n",
        "    doc = {}\n",
        "    if redundantes:\n",
        "        doc[\"redundant\"] = redundantes\n",
        "        print(f\"[INFO] Features redundantes: {redundantes}\")\n",
        "    if irrelevantes:\n",
        "        doc[\"irrelevant\"] = irrelevantes\n",
        "        print(f\"[INFO] Features irrelevantes: {irrelevantes}\")\n",
        "    return doc\n",
        "\n",
        "# [7.2] Limpieza y Agrupamiento de Categor\u00edas en Variables Categ\u00f3ricas\n",
        "def propose_category_groupings(\n",
        "    df: pd.DataFrame,\n",
        "    cat_cols: list,\n",
        "    min_freqs: dict = None,\n",
        "    default_min_freq: int = 10,\n",
        "    other_label: str = \"Other\"\n",
        ") -> dict:\n",
        "    \"\"\"\n",
        "    Permite definir min_freq distinto por columna (usando un dict), o usar uno por defecto.\n",
        "    \"\"\"\n",
        "    groupings = {}\n",
        "    for col in cat_cols:\n",
        "        min_freq = min_freqs[col] if min_freqs and col in min_freqs else default_min_freq\n",
        "        freqs = df[col].value_counts()\n",
        "        rare = freqs[freqs < min_freq].index.tolist()\n",
        "        if rare:\n",
        "            groupings[col] = {\n",
        "                \"rare_categories\": rare,\n",
        "                \"min_freq\": min_freq,\n",
        "                \"other_label\": other_label\n",
        "            }\n",
        "            print(f\"[INFO] En '{col}' se agrupar\u00e1n {len(rare)} categor\u00edas poco frecuentes como '{other_label}' (min_freq={min_freq}).\")\n",
        "    return groupings\n",
        "\n",
        "# [7.3] Documentaci\u00f3n y Preparaci\u00f3n de Features Finales\n",
        "def prepare_final_features(df: pd.DataFrame, drop_features: list, target_col: str) -> dict:\n",
        "    \"\"\"\n",
        "    Define y documenta las listas finales de features para modelado.\n",
        "    \"\"\"\n",
        "    cols = [c for c in df.columns if c not in drop_features + [target_col]]\n",
        "    num_features = df[cols].select_dtypes(include='number').columns.tolist()\n",
        "    cat_features = [c for c in cols if c not in num_features]\n",
        "    features_dict = {\n",
        "        \"final_features\": cols,\n",
        "        \"final_num_features\": num_features,\n",
        "        \"final_cat_features\": cat_features\n",
        "    }\n",
        "    print(f\"[INFO] Features finales para modelado: {features_dict}\")\n",
        "    return features_dict\n",
        "\n",
        "# [7.4] Identificaci\u00f3n de Problemas Potenciales para el Pipeline\n",
        "def collect_pipeline_issues(df: pd.DataFrame, features_dict: dict) -> dict:\n",
        "    \"\"\"\n",
        "    Documenta problemas potenciales y necesidades de preprocesamiento para el pipeline.\n",
        "    \"\"\"\n",
        "    issues = {}\n",
        "    # NaNs\n",
        "    for col in features_dict[\"final_num_features\"]:\n",
        "        n_nans = df[col].isnull().sum()\n",
        "        if n_nans > 0:\n",
        "            issues[col] = {\"nans\": n_nans, \"type\": \"numeric\"}\n",
        "    for col in features_dict[\"final_cat_features\"]:\n",
        "        n_nans = df[col].isnull().sum()\n",
        "        if n_nans > 0:\n",
        "            issues[col] = {\"nans\": n_nans, \"type\": \"categorical\"}\n",
        "    # Cardinalidad alta\n",
        "    for col in features_dict[\"final_cat_features\"]:\n",
        "        n_unique = df[col].nunique()\n",
        "        if n_unique > 20:  # threshold arbitrario, ajustable\n",
        "            issues[col] = issues.get(col, {})\n",
        "            issues[col].update({\"high_cardinality\": n_unique})\n",
        "    print(f\"[INFO] Problemas/preprocesamientos detectados para el pipeline: {issues}\")\n",
        "    return issues\n",
        "\n",
        "# [7.5] Logging y Guardado de Configuraci\u00f3n de Features\n",
        "def save_feature_config(features_dict: dict, config_path: str) -> None:\n",
        "    \"\"\"\n",
        "    Guarda y documenta la configuraci\u00f3n final de features.\n",
        "    \"\"\"\n",
        "    with open(config_path, \"w\") as f:\n",
        "        json.dump(features_dict, f, indent=2)\n",
        "    print(f\"[INFO] Configuraci\u00f3n de features guardada en {config_path}\")\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 7.1 An\u00e1lisis y documentaci\u00f3n de features\n",
        "features_info = {\n",
        "    \"redundant\": [],  # Ejemplo: ['education'] si no la has dropeado ya\n",
        "    \"irrelevant\": []  # Ejemplo: ['fnlwgt']\n",
        "}\n",
        "redundancy_doc = analyze_feature_redundancy_irrelevance(df, features_info)\n",
        "\n",
        "# 7.2 Propuesta de agrupamiento de categor\u00edas poco frecuentes (SEGUN DATOS DE EDA)\n",
        "min_freqs = {\n",
        "    \"native.country\": 1000,\n",
        "    \"occupation\": 2000,\n",
        "    \"race\": 500,\n",
        "    'workclass': 3000,\n",
        "    'marital.status': 2000,\n",
        "    'relationship': 4000, \n",
        "}\n",
        "category_groupings = propose_category_groupings(\n",
        "    df,\n",
        "    config[\"cat_features\"],\n",
        "    min_freqs=min_freqs,\n",
        "    default_min_freq=10  # Puedes ajustar seg\u00fan el EDA o dejarlo as\u00ed si tu config ya agrupa con thresholds personalizados\n",
        ")\n",
        "\n",
        "# 7.3 Definici\u00f3n y documentaci\u00f3n de features finales para modelado\n",
        "features_dict = prepare_final_features(\n",
        "    df,\n",
        "    config[\"drop_features\"],\n",
        "    config[\"target\"]\n",
        ")\n",
        "# (Esto te da listas: final_features, final_num_features, final_cat_features)\n",
        "\n",
        "# 7.4 Identificaci\u00f3n de problemas potenciales para el pipeline\n",
        "pipeline_issues = collect_pipeline_issues(df, features_dict)\n",
        "\n",
        "# 7.5 Guardado de la configuraci\u00f3n de features\n",
        "save_feature_config(features_dict, \"features_config.json\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ### BLOQUE 8: Preparaci\u00f3n de X/y y Split Estratificado\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# [8.1] Preparaci\u00f3n de Features y Target para el Split\n",
        "def prepare_X_y(df: pd.DataFrame, features_dict: dict, target_col: str) -> (pd.DataFrame, pd.Series):\n",
        "    \"\"\"\n",
        "    Prepara X (features) e y (target) a partir de las listas definidas.\n",
        "    \"\"\"\n",
        "    # Puedes adaptar keys seg\u00fan tu features_dict final:\n",
        "    features = features_dict.get(\"final_features\", [])\n",
        "    X = df[features].copy()\n",
        "    y = df[target_col].copy()\n",
        "    print(f\"[INFO] Matriz X preparada (shape: {X.shape}), vector y (shape: {y.shape})\")\n",
        "    return X, y\n",
        "\n",
        "# [8.2] Divisi\u00f3n Reproducible y Estratificada\n",
        "def stratified_train_test_split(X: pd.DataFrame, y: pd.Series, test_size: float = 0.2, random_state: int = 42) -> dict:\n",
        "    \"\"\"\n",
        "    Realiza el split train/test estratificado y reproducible.\n",
        "    \"\"\"\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=test_size, random_state=random_state, stratify=y\n",
        "    )\n",
        "    print(f\"[INFO] Split estratificado hecho. X_train: {X_train.shape}, X_test: {X_test.shape}\")\n",
        "    return {\n",
        "        \"X_train\": X_train,\n",
        "        \"X_test\": X_test,\n",
        "        \"y_train\": y_train,\n",
        "        \"y_test\": y_test\n",
        "    }\n",
        "\n",
        "# [8.3] Chequeos de Calidad Post-Split\n",
        "def check_split_quality(y_train: pd.Series, y_test: pd.Series) -> dict:\n",
        "    \"\"\"\n",
        "    Verifica la distribuci\u00f3n del target en train y test y chequea posibles anomal\u00edas.\n",
        "    \"\"\"\n",
        "    train_dist = y_train.value_counts(normalize=True).to_dict()\n",
        "    test_dist = y_test.value_counts(normalize=True).to_dict()\n",
        "    n_train, n_test = len(y_train), len(y_test)\n",
        "    n_train_null = y_train.isna().sum()\n",
        "    n_test_null = y_test.isna().sum()\n",
        "    obs = {\n",
        "        \"train_dist\": train_dist,\n",
        "        \"test_dist\": test_dist,\n",
        "        \"n_train\": n_train,\n",
        "        \"n_test\": n_test,\n",
        "        \"n_train_null\": n_train_null,\n",
        "        \"n_test_null\": n_test_null\n",
        "    }\n",
        "    print(f\"[INFO] Distribuci\u00f3n de clases (train): {train_dist}\")\n",
        "    print(f\"[INFO] Distribuci\u00f3n de clases (test): {test_dist}\")\n",
        "    if n_train_null or n_test_null:\n",
        "        print(f\"[WARN] Nulos en splits - train: {n_train_null}, test: {n_test_null}\")\n",
        "    return obs\n",
        "\n",
        "# [8.4] Guardado y Logging del Estado Post-Split\n",
        "def log_and_save_split_state(split_dict: dict, out_dir: str = None) -> None:\n",
        "    \"\"\"\n",
        "    Loguea y guarda el estado post-split.\n",
        "    \"\"\"\n",
        "    for k, v in split_dict.items():\n",
        "        print(f\"[INFO] {k}: shape {v.shape}\")\n",
        "    if out_dir:\n",
        "        import os\n",
        "        os.makedirs(out_dir, exist_ok=True)\n",
        "        for k, v in split_dict.items():\n",
        "            save_path = f\"{out_dir}/{k}.csv\"\n",
        "            v.to_csv(save_path, index=False)\n",
        "            print(f\"[INFO] {k} guardado en {save_path}\") \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 8.1 Preparaci\u00f3n de features y target para el split\n",
        "X, y = prepare_X_y(df, features_dict, config[\"target\"])\n",
        "\n",
        "# 8.2 Divisi\u00f3n reproducible y estratificada\n",
        "splits = stratified_train_test_split(\n",
        "    X,\n",
        "    y,\n",
        "    test_size=config[\"split\"][\"test_size\"],\n",
        "    random_state=config[\"split\"][\"random_state\"]\n",
        ")\n",
        "\n",
        "# 8.3 Chequeo de calidad post-split (verifica balance de clases y posibles nulos)\n",
        "split_quality = check_split_quality(splits[\"y_train\"], splits[\"y_test\"])\n",
        "\n",
        "# 8.4 Logging y guardado del estado post-split (opcional)\n",
        "log_and_save_split_state(splits, out_dir=\"split_artifacts\")\n",
        "\n",
        "X_train = splits[\"X_train\"]\n",
        "X_test = splits[\"X_test\"]\n",
        "y_train = splits[\"y_train\"]\n",
        "y_test = splits[\"y_test\"]\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "desc_num = describe_numerical(X_train, config[\"num_features\"])      # Resvisamos como se ve el train set\n",
        "desc_cat = describe_categorical(X_train, config[\"cat_features\"])\n",
        "plot_numerical_distributions(X_train, config[\"num_features\"])\n",
        "\n",
        "# 6.3 Visualizaci\u00f3n de Variables Categ\u00f3ricas (agrupando categor\u00edas poco frecuentes)\n",
        "plot_categorical_distributions(X_train, config[\"cat_features\"], min_freq=10)\n",
        "\n",
        "# 6.4 Relaci\u00f3n Univariada de Features con el Target\n",
        "plot_feature_target_relationship(X_train, config[\"features\"], config[\"target\"])\n",
        "\n",
        "# 6.5 Correlaciones y Relaciones Notables\n",
        "corr_matrix = correlation_analysis(X_train, config[\"num_features\"])\n",
        "redundancies = analyze_feature_redundancy(X_train, config[\"num_features\"])\n",
        "\n",
        "# 6.6 (Opcional) Documentaci\u00f3n de Hallazgos del EDA\n",
        "findings = {\n",
        "    \"desc_num\": desc_num.to_dict(),\n",
        "    \"desc_cat\": desc_cat.to_dict(),\n",
        "    \"correlaciones\": corr_matrix.to_dict(),\n",
        "    \"redundancias\": redundancies,\n",
        "    # Puedes agregar insights manualmente despu\u00e9s de revisar visualizaciones\n",
        "}\n",
        "document_eda_findings(findings, save_path=\"eda_findings.txt\")\n",
        "\n",
        "# 6.7 (Opcional) Guardado de artefactos de EDA\n",
        "# (Si quieres guardar gr\u00e1ficos/tablas, deber\u00edas recolectarlos en listas)\n",
        "# save_eda_artifacts(plots, tables, out_dir=\"eda_artifacts\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ### BLOQUE 9: Wrappers, ColumnTransformer y Pipeline Modular\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# === BLOQUE 9: WRAPPERS, COLUMNTRANSFORMER Y PIPELINE MODULAR ===\n",
        "\n",
        "# --------- Wrappers Custom (sklearn friendly, tunables) ---------\n",
        "\n",
        "class OutlierTransformer(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    Detecta y trata outliers en columnas num\u00e9ricas.\n",
        "    Estrategias soportadas: \"clip\" (winsorize), \"none\".\n",
        "    Par\u00e1metros tunables:\n",
        "        - strategy: {\"clip\", \"none\"}\n",
        "        - pmin: percentil m\u00ednimo (ej: 0.01)\n",
        "        - pmax: percentil m\u00e1ximo (ej: 0.99)\n",
        "    \"\"\"\n",
        "    def __init__(self, strategy=\"clip\", pmin=0.01, pmax=0.99):\n",
        "        self.strategy = strategy\n",
        "        self.pmin = pmin\n",
        "        self.pmax = pmax\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        if self.strategy == \"clip\":\n",
        "            # Guarda l\u00edmites por columna\n",
        "            self.limits_ = {\n",
        "                col: (\n",
        "                    X[col].quantile(self.pmin),\n",
        "                    X[col].quantile(self.pmax)\n",
        "                ) for col in X.columns\n",
        "            }\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X_out = X.copy()\n",
        "        if self.strategy == \"clip\" and hasattr(self, \"limits_\"):\n",
        "            for col, (minv, maxv) in self.limits_.items():\n",
        "                X_out[col] = X_out[col].clip(minv, maxv)\n",
        "        return X_out\n",
        "\n",
        "class CategoryGrouper(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    Agrupa categor\u00edas poco frecuentes en un valor 'Other' seg\u00fan min_freq.\n",
        "    Pensado para trabajar con una sola columna (por pipeline/ColumnTransformer).\n",
        "    Par\u00e1metros tunables:\n",
        "        - min_freq: m\u00ednima frecuencia absoluta para NO agrupar\n",
        "        - other_label: etiqueta para categor\u00edas agrupadas\n",
        "    \"\"\"\n",
        "    def __init__(self, col, min_freq=10, other_label=\"Other\"):\n",
        "        self.col = col\n",
        "        self.min_freq = min_freq\n",
        "        self.other_label = other_label\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        # X puede ser DataFrame (N,1) o array (N,1)\n",
        "        if hasattr(X, \"iloc\"):\n",
        "            vals = X.iloc[:, 0].value_counts()\n",
        "        else:\n",
        "            import numpy as np\n",
        "            vals = pd.Series(X[:, 0]).value_counts()\n",
        "        self.major_cats_ = vals[vals >= self.min_freq].index.tolist()\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        # Aplica el agrupamiento solo a la columna correspondiente\n",
        "        if hasattr(X, \"iloc\"):\n",
        "            X_ = X.copy()\n",
        "            X_.iloc[:, 0] = X_.iloc[:, 0].apply(lambda x: x if x in self.major_cats_ else self.other_label)\n",
        "            return X_\n",
        "        else:\n",
        "            import numpy as np\n",
        "            X_ = X.copy()\n",
        "            X_[:, 0] = np.where(np.isin(X_[:, 0], self.major_cats_), X_[:, 0], self.other_label)\n",
        "            return X_\n",
        "\n",
        "# --------- F\u00e1brica de ColumnTransformer (totalmente parametrizable) ---------\n",
        "\n",
        "def build_column_transformer(config: dict, features_dict: dict) -> ColumnTransformer:\n",
        "    \"\"\"\n",
        "    Construye el ColumnTransformer de preprocesamiento seg\u00fan config y features_dict.\n",
        "    Exponible a hiperpar\u00e1metros para GridSearchCV.\n",
        "    \"\"\"\n",
        "    transformers = []\n",
        "\n",
        "    # --- Pipeline num\u00e9rico ---\n",
        "    num_steps = []\n",
        "    if config.get(\"outliers\", {}).get(\"strategy\", \"none\") != \"none\":\n",
        "        num_steps.append(\n",
        "            (\"outlier\", OutlierTransformer(\n",
        "                strategy=config[\"outliers\"][\"strategy\"],\n",
        "                pmin=config[\"outliers\"][\"params\"][\"pmin\"],\n",
        "                pmax=config[\"outliers\"][\"params\"][\"pmax\"]\n",
        "            ))\n",
        "        )\n",
        "    num_steps.append((\"imputer\", SimpleImputer(strategy=config[\"imputation\"][\"num_method\"])))\n",
        "\n",
        "    scaling = config.get(\"scaling\", \"standard\")\n",
        "    if scaling == \"standard\":\n",
        "        num_steps.append((\"scaler\", StandardScaler()))\n",
        "    elif scaling == \"minmax\":\n",
        "        num_steps.append((\"scaler\", MinMaxScaler()))\n",
        "    elif scaling == \"robust\":\n",
        "        num_steps.append((\"scaler\", RobustScaler()))\n",
        "\n",
        "    transformers.append(\n",
        "        (\"num\", Pipeline(num_steps), features_dict[\"final_num_features\"])\n",
        "    )\n",
        "\n",
        "    # --- Pipeline categ\u00f3rico (uno por columna) ---\n",
        "    for cat in features_dict[\"final_cat_features\"]:\n",
        "        steps = []\n",
        "        # Imputaci\u00f3n\n",
        "        cat_method = config[\"imputation\"][\"cat_method\"]\n",
        "\n",
        "        if cat_method == \"constant\":\n",
        "            steps.append((\n",
        "                \"imputer\", \n",
        "                SimpleImputer(\n",
        "                    strategy=\"constant\",\n",
        "                    fill_value=config[\"imputation\"].get(\"cat_fill_value\", \"Missing\")\n",
        "                )\n",
        "            ))\n",
        "        else:\n",
        "            steps.append((\n",
        "            \"imputer\", \n",
        "            SimpleImputer(strategy=config[\"imputation\"][\"cat_method\"])\n",
        "        ))\n",
        "\n",
        "        # Agrupamiento, solo si est\u00e1 en config[\"grouping\"]\n",
        "        grouping_cfg = config.get(\"grouping\", {}).get(cat, None)\n",
        "        if grouping_cfg is not None:\n",
        "            steps.append((\n",
        "                \"grouper\", \n",
        "                CategoryGrouper(\n",
        "                    col=cat,\n",
        "                    min_freq=grouping_cfg.get(\"min_freq\", 10),\n",
        "                    other_label=grouping_cfg.get(\"other_label\", \"Other\")\n",
        "                )\n",
        "            ))\n",
        "        # Encoding (solo OneHot, puedes agregar Ordinal f\u00e1cilmente)\n",
        "        steps.append((\"encoder\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)))\n",
        "        transformers.append(\n",
        "            (f\"cat_{cat}\", Pipeline(steps), [cat])\n",
        "        )\n",
        "\n",
        "    col_trans = ColumnTransformer(transformers=transformers, remainder=\"drop\")\n",
        "    print(f\"[INFO] ColumnTransformer construido con {len(transformers)} transformadores.\")\n",
        "    return col_trans\n",
        "\n",
        "# --------- F\u00e1brica de Pipeline (modelo tambi\u00e9n parametrizable) ---------\n",
        "\n",
        "def build_pipeline(config: dict, features_dict: dict):\n",
        "    \"\"\"\n",
        "    Construye el pipeline completo: preprocesamiento + modelo, seg\u00fan config y features_dict.\n",
        "    Tunable y compatible con GridSearchCV.\n",
        "    \"\"\"\n",
        "    col_trans = build_column_transformer(config, features_dict)\n",
        "    model_type = config[\"model\"][\"type\"]\n",
        "    model_params = config[\"model\"][\"params\"]\n",
        "\n",
        "    if model_type == \"LogisticRegression\":\n",
        "        model = LogisticRegression(**model_params)\n",
        "    elif model_type == \"DecisionTreeClassifier\":\n",
        "        model = DecisionTreeClassifier(**model_params)\n",
        "    elif model_type == \"RandomForestClassifier\":\n",
        "        model = RandomForestClassifier(**model_params)\n",
        "    # Agrega aqu\u00ed otros modelos si lo deseas...\n",
        "    else:\n",
        "        raise NotImplementedError(f\"Modelo no implementado: {model_type}\")\n",
        "\n",
        "    pipe = Pipeline([\n",
        "        (\"preprocessing\", col_trans),\n",
        "        (\"model\", model)\n",
        "    ])\n",
        "    print(f\"[INFO] Pipeline completo construido: {model_type}\")\n",
        "    return pipe\n",
        "\n",
        "# --------- F\u00e1brica r\u00e1pida para experimentos ---------\n",
        "\n",
        "def make_pipeline_for_experiment(config: dict, features_dict: dict):\n",
        "    \"\"\"\n",
        "    Alias r\u00e1pido para construcci\u00f3n de pipeline, usado en experimentos.\n",
        "    \"\"\"\n",
        "    return build_pipeline(config, features_dict)\n",
        "\n",
        "# [9.loquesea] Serealiza objetos no serealizables a Json\n",
        " \n",
        "def make_json_serializable(d):\n",
        "    \"\"\"\n",
        "    Convierte todos los valores de un dict a algo serializable por JSON.\n",
        "    Los objetos sklearn (como StandardScaler) se pasan a str.\n",
        "    \"\"\"\n",
        "    out = {}\n",
        "    for k, v in d.items():\n",
        "        # Si es un objeto de sklearn o similar, pasalo a string\n",
        "        try:\n",
        "            json.dumps(v)\n",
        "            out[k] = v\n",
        "        except TypeError:\n",
        "            out[k] = str(v)\n",
        "    return out\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ### BLOQUE 10: MLflow, M\u00e9tricas, Experimentos y Comparaci\u00f3n\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# === BLOQUE 10: EXPERIMENTACI\u00d3N AUTOM\u00c1TICA Y MLflow AUTOLOG ===\n",
        "\n",
        "def setup_mlflow_experiment(tracking_uri: str, experiment_name: str):\n",
        "    \"\"\"\n",
        "    Configura MLflow: URI y nombre del experimento.\n",
        "    Si no existe, lo crea.\n",
        "    \"\"\"\n",
        "    mlflow.set_tracking_uri(tracking_uri)\n",
        "    mlflow.set_experiment(experiment_name)\n",
        "    print(f\"[INFO] MLflow configurado: {tracking_uri}, experimento: {experiment_name}\")\n",
        "\n",
        "def run_gridsearch_experiment(\n",
        "    pipeline, param_grid, X_train, y_train, X_test, y_test,\n",
        "    scoring=\"accuracy\", cv=5, out_dir=\"results_out\", tags=None\n",
        "):\n",
        "    \"\"\"\n",
        "    Lanza GridSearchCV sobre el pipeline y trackea todo autom\u00e1ticamente en MLflow.\n",
        "    Guarda el mejor modelo y el config usados como artefactos.\n",
        "    \"\"\"\n",
        "    import os, json, joblib\n",
        "\n",
        "    mlflow.sklearn.autolog()  # \u00a1clave! Trackea todo el GridSearch autom\u00e1ticamente\n",
        "\n",
        "    with mlflow.start_run(tags=tags):\n",
        "        # Lanzamos GridSearchCV\n",
        "        grid = GridSearchCV(\n",
        "            estimator=pipeline,\n",
        "            param_grid=param_grid,\n",
        "            scoring=\"accuracy\",\n",
        "            cv=5,\n",
        "            n_jobs=-1,\n",
        "            verbose=2,\n",
        "            return_train_score=True\n",
        "        )\n",
        "        grid.fit(X_train, y_train)\n",
        "\n",
        "        # Loguea los resultados y el mejor modelo\n",
        "        best_model = grid.best_estimator_\n",
        "        best_params = grid.best_params_\n",
        "        best_score = grid.best_score_\n",
        "        print(\"[INFO] Mejor score (cv):\", best_score)\n",
        "        print(\"[INFO] Mejores hiperpar\u00e1metros:\", best_params)\n",
        "\n",
        "        # Loguea score en test set\n",
        "        test_score = grid.score(X_test, y_test)\n",
        "        mlflow.log_metric(\"test_score\", test_score)\n",
        "\n",
        "        # Guarda artefactos extra: modelo, params, resultados de GridSearch\n",
        "        os.makedirs(out_dir, exist_ok=True)\n",
        "        joblib.dump(best_model, f\"{out_dir}/best_model.joblib\")\n",
        "        with open(f\"{out_dir}/best_params.json\", \"w\") as f:\n",
        "            json.dump(make_json_serializable(best_params), f, indent=2)\n",
        "\n",
        "\n",
        "        # GridSearchCV cv_results_\n",
        "        pd.DataFrame(grid.cv_results_).to_csv(f\"{out_dir}/cv_results.csv\", index=False)\n",
        "        mlflow.log_artifacts(out_dir)\n",
        "\n",
        "    print(f\"[INFO] Experimento terminado y trackeado en MLflow.\")\n",
        "\n",
        "def make_json_serializable(d):\n",
        "    \"\"\"\n",
        "    Convierte todos los valores de un dict a algo serializable por JSON.\n",
        "    Los objetos sklearn (como StandardScaler) se pasan a str.\n",
        "    \"\"\"\n",
        "    out = {}\n",
        "    for k, v in d.items():\n",
        "        # Si es un objeto de sklearn o similar, pasalo a string\n",
        "        try:\n",
        "            json.dumps(v)\n",
        "            out[k] = v\n",
        "        except TypeError:\n",
        "            out[k] = str(v)\n",
        "    return out\n",
        "\n",
        "\n",
        "# Ejemplo de uso (pseudo-c\u00f3digo):\n",
        "\"\"\"\n",
        "setup_mlflow_experiment(config[\"mlflow\"][\"tracking_uri\"], config[\"mlflow\"][\"experiment_name\"])\n",
        "\n",
        "param_grid = {\n",
        "    # Parametriza hiperpar\u00e1metros del pipeline y modelo\n",
        "    \"preprocessing__num__outlier__pmin\": [0.01, 0.05],\n",
        "    \"preprocessing__num__outlier__pmax\": [0.95, 0.99],\n",
        "    \"preprocessing__num__scaler\": [StandardScaler(), RobustScaler()],\n",
        "    \"model__C\": [0.1, 1, 10],\n",
        "    # ...agrega otros hiperpar\u00e1metros aqu\u00ed...\n",
        "}\n",
        "\n",
        "pipeline = make_pipeline_for_experiment(config, features_dict)\n",
        "\n",
        "run_gridsearch_experiment(\n",
        "    pipeline, param_grid,\n",
        "    X_train, y_train, X_test, y_test,\n",
        "    scoring=\"accuracy\",\n",
        "    cv=5,\n",
        "    out_dir=\"results_out\"\n",
        ")\n",
        "\"\"\"\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# === PLANTILLA 00 EXPERIMENTO END-TO-END ===\n",
        "\n",
        "# 1. Preparaci\u00f3n del dict de features (puede venir de tu EDA o selecci\u00f3n previa)\n",
        "features_dict = {\n",
        "    \"final_num_features\": config[\"num_features\"],         # ajusta si haces selecci\u00f3n de features\n",
        "    \"final_cat_features\": config[\"cat_features\"],         # ajusta si haces selecci\u00f3n de features\n",
        "}\n",
        "\n",
        "# 2. Construcci\u00f3n del pipeline tunable\n",
        "# pipeline = make_pipeline_for_experiment(config, features_dict)\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    (\"preprocessing\", build_column_transformer(config, features_dict)),\n",
        "    (\"model\", LogisticRegression()) # Placeholder, ser\u00e1 sobreescrito por GridSearchCV\n",
        "])\n",
        "\n",
        "# 3. Definici\u00f3n del espacio de hiperpar\u00e1metros (param_grid)\n",
        "\n",
        "param_grid = [\n",
        "    # Logistic Regression\n",
        "    {\n",
        "        \"model\": [LogisticRegression(max_iter=1000, solver=\"lbfgs\")],\n",
        "        \"model__C\": [0.1, 1, 10, 100],\n",
        "        \"model__penalty\": [\"l2\"],\n",
        "        \"preprocessing__num__outlier__pmin\": [0.01, 0.05],\n",
        "        \"preprocessing__num__outlier__pmax\": [0.95, 0.99],\n",
        "        \"preprocessing__num__outlier__strategy\": [\"clip\", \"none\"],\n",
        "        \"preprocessing__num__scaler\": [StandardScaler(), RobustScaler()],\n",
        "        \n",
        "        # \"preprocessing__cat_native.country__grouper__min_freq\": [50, 100, 200],\n",
        "        # \"preprocessing__cat_workclass__grouper__min_freq\": [1000, 2000, 3000],\n",
        "        # \"preprocessing__cat_occupation__grouper__min_freq\": [1000, 2000],\n",
        "        # \"preprocessing__cat_relationship__grouper__min_freq\": [1000, 2000],\n",
        "        # \"preprocessing__cat_marital.status__grouper__min_freq\": [1000, 2000],\n",
        "        # \"preprocessing__cat_race__grouper__min_freq\": [1000, 2000],\n",
        "        \n",
        "\n",
        "    },\n",
        "    # Decision Tree\n",
        "    {\n",
        "        \"model\": [DecisionTreeClassifier()],\n",
        "        \"model__max_depth\": [3, 5, 10, None],\n",
        "        \"model__min_samples_split\": [2, 10, 20],\n",
        "        \"preprocessing__num__outlier__pmin\": [0.01, 0.05],\n",
        "        \"preprocessing__num__outlier__pmax\": [0.95, 0.99],\n",
        "        \"preprocessing__num__outlier__strategy\": [\"clip\", \"none\"],\n",
        "        \"preprocessing__num__scaler\": [StandardScaler(), RobustScaler()],\n",
        "    },\n",
        "    # Random Forest\n",
        "    {\n",
        "        \"model\": [RandomForestClassifier(n_estimators=100)],\n",
        "        \"model__max_depth\": [5, 10, 20, None],\n",
        "        \"model__min_samples_split\": [2, 10],\n",
        "        \"preprocessing__num__outlier__pmin\": [0.01, 0.05],\n",
        "        \"preprocessing__num__outlier__pmax\": [0.95, 0.99],\n",
        "        \"preprocessing__num__outlier__strategy\": [\"clip\", \"none\"],\n",
        "        \"preprocessing__num__scaler\": [StandardScaler(), RobustScaler()],\n",
        "    }\n",
        "]\n",
        "\n",
        "# 5. Setup MLflow (URI y experimento vienen del config)\n",
        "setup_mlflow_experiment(\n",
        "    tracking_uri=config[\"mlflow\"][\"tracking_uri\"],\n",
        "    experiment_name=config[\"mlflow\"][\"experiment_name\"]\n",
        ")\n",
        "\n",
        "# 6. Lanza el experimento completo y trackea todo\n",
        "run_gridsearch_experiment(\n",
        "    pipeline, param_grid,\n",
        "    X_train, y_train, X_test, y_test,\n",
        "    scoring=\"accuracy\",      # Cambia a roc_auc, f1, etc. seg\u00fan tu m\u00e9trica objetivo\n",
        "    cv=5,                    # Folds de CV\n",
        "    out_dir=\"results_out\",\n",
        "    tags={\"proyecto\": \"test0\", \"autor\": \"Javiercito\"}\n",
        ")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# === PLANTILLA 01 EXPERIMENTO END-TO-END ===\n",
        "\n",
        "# 1. Preparaci\u00f3n del dict de features (puede venir de tu EDA o selecci\u00f3n previa)\n",
        "features_dict = {\n",
        "    \"final_num_features\": config[\"num_features\"],         # ajusta si haces selecci\u00f3n de features\n",
        "    \"final_cat_features\": config[\"cat_features\"],         # ajusta si haces selecci\u00f3n de features\n",
        "}\n",
        "\n",
        "# 2. Construcci\u00f3n del pipeline tunable\n",
        "# pipeline = make_pipeline_for_experiment(config, features_dict)\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    (\"preprocessing\", build_column_transformer(config, features_dict)),\n",
        "    (\"model\", LogisticRegression()) # Placeholder, ser\u00e1 sobreescrito por GridSearchCV\n",
        "])\n",
        "\n",
        "# 3. Definici\u00f3n del espacio de hiperpar\u00e1metros (param_grid)\n",
        "\n",
        "param_grid = [\n",
        "    {\n",
        "        \"model\": [RandomForestClassifier(n_estimators=100, random_state=42)],\n",
        "        \"model__max_depth\": [10, 15, 20, None],\n",
        "        \"model__min_samples_split\": [10, 20, 50],\n",
        "        # (Opcional)\n",
        "        # \"model__max_features\": [\"auto\", \"sqrt\", 0.5],\n",
        "\n",
        "        # Preprocesamiento num\u00e9rico fijo\n",
        "        \"preprocessing__num__outlier__strategy\": [\"none\"],\n",
        "        \"preprocessing__num__scaler\": [RobustScaler()],\n",
        "    }\n",
        "]\n",
        "\n",
        "# 5. Setup MLflow (URI y experimento vienen del config)\n",
        "setup_mlflow_experiment(\n",
        "    tracking_uri=config[\"mlflow\"][\"tracking_uri\"],\n",
        "    experiment_name=config[\"mlflow\"][\"experiment_name\"]\n",
        ")\n",
        "\n",
        "# 6. Lanza el experimento completo y trackea todo\n",
        "run_gridsearch_experiment(\n",
        "    pipeline, param_grid,\n",
        "    X_train, y_train, X_test, y_test,\n",
        "    scoring=\"accuracy\",      # Cambia a roc_auc, f1, etc. seg\u00fan tu m\u00e9trica objetivo\n",
        "    cv=5,                    # Folds de CV\n",
        "    out_dir=\"results_out\",\n",
        "    tags={\"proyecto\": \"test1\", \"autor\": \"Javiercito\"}\n",
        ")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# === PLANTILLA 02 EXPERIMENTO END-TO-END ===\n",
        "\n",
        "# Configuraci\u00f3n robusta identificada\n",
        "pipeline = Pipeline([\n",
        "    (\"preprocessing\", build_column_transformer(config, features_dict)),\n",
        "    (\"model\", RandomForestClassifier(\n",
        "        n_estimators=100,\n",
        "        max_depth=15,\n",
        "        min_samples_split=20,\n",
        "        random_state=42\n",
        "    ))\n",
        "])\n",
        "\n",
        "# Param_grid para grouping de dos categ\u00f3ricas\n",
        "param_grid = [\n",
        "    {\n",
        "        # Grouping tunable\n",
        "        \"preprocessing__cat_native.country__grouper__min_freq\": [5000],\n",
        "        \"preprocessing__cat_workclass__grouper__min_freq\": [3000],\n",
        "        \"preprocessing__cat_occupation__grouper__min_freq\": [2000, 5000, 7000],\n",
        "        \"preprocessing__cat_relationship__grouper__min_freq\": [4000, 6000],\n",
        "        \"preprocessing__cat_marital.status__grouper__min_freq\": [2000, 3000, 4000],\n",
        "        \"preprocessing__cat_race__grouper__min_freq\": [2000],\n",
        "        # Preprocesamiento num\u00e9rico fijo\n",
        "        \"preprocessing__num__outlier__strategy\": [\"none\"],\n",
        "        \"preprocessing__num__scaler\": [RobustScaler()],\n",
        "    }\n",
        "]\n",
        "\n",
        "# ... El resto igual\n",
        "run_gridsearch_experiment(\n",
        "    pipeline, param_grid,\n",
        "    X_train, y_train, X_test, y_test,\n",
        "    scoring=\"accuracy\",\n",
        "    cv=5,\n",
        "    out_dir=\"results_out_grouping\",\n",
        "    tags={\"proyecto\": \"rf_grouping\", \"autor\": \"Javiercito\"}\n",
        ")\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
